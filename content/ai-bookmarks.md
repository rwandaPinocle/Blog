---
title: "Interesting AI Tools and Projects"
date: 2024-06-11T11:26:08-07:00
draft: false
---

[Nemotron 4, 340B](https://huggingface.co/collections/nvidia/nemotron-4-340b-666b7ebaf1b3867caf2f1911):
Nvidia's LLM that is roughly the same size as GPT-4, with public weights.

[Stable-Diffusion.mojo](https://github.com/lrmantovani10/Stable-Diffusion.mojo):
An implementation of the forward pass of Tiny Stable Diffusion in Mojo.
Includes an implementation of CLIP in Mojo as well.

[Dragonfly](https://www.together.ai/blog/dragonfly-v1):
A vision and text LLM derived from Llama3, outperforms LLaVA and QWEN

[Qwen2](https://qwenlm.github.io/blog/qwen2/):
Open LLM 70B params, beats Lllama 3

[Wonnx](https://github.com/webonnx/wonnx):
Onnx runtime for WebGPU

[OpenVINO](https://github.com/openvinotoolkit/openvino):
Intel's toolkit for deploying DL models

[ONNX-MLIR](https://github.com/onnx/onnx-mlir):
A tool for compiling ONNX models down to code

[Pytorch MLIR](https://github.com/llvm/torch-mlir)

[TVM](https://github.com/apache/tvm/):
A neural network compiler

[MLP-Mixer](https://arxiv.org/pdf/2105.01601v4):
An MLP based architecture that matches transformer performance

[CLIP.cpp](https://github.com/monatis/clip.cpp/tree/main):
A C++ implementation of CLIP

Training on a single gpu
 - https://huggingface.co/docs/transformers/en/perf_train_gpu_one
 - https://magazine.sebastianraschka.com/p/understanding-large-language-models
 - https://towardsdatascience.com/editing-text-in-images-with-ai-03dee75d8b9c
 - https://github.com/Mozilla-Ocho/llamafile


